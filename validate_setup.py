#!/usr/bin/env python3
"""
Script de Validaci√≥n Pre-Ejecuci√≥n
====================================

Verifica que todo est√© listo para ejecutar la evaluaci√≥n completa de la tesis.

Uso:
    python validate_setup.py
"""

import sys
import subprocess
from pathlib import Path

# Colores
GREEN = '\033[0;32m'
RED = '\033[0;31m'
YELLOW = '\033[1;33m'
BLUE = '\033[0;34m'
NC = '\033[0m'

def check_item(name, check_func):
    """Verifica un item y muestra resultado."""
    try:
        result = check_func()
        if result:
            print(f"{GREEN}‚úÖ {name}{NC}")
            return True
        else:
            print(f"{RED}‚ùå {name}{NC}")
            return False
    except Exception as e:
        print(f"{RED}‚ùå {name} - Error: {e}{NC}")
        return False

def check_file_exists(file_path):
    """Verifica que un archivo exista."""
    return lambda: Path(file_path).exists()

def check_module(module_name):
    """Verifica que un m√≥dulo de Python est√© instalado."""
    def check():
        try:
            __import__(module_name)
            return True
        except ImportError:
            return False
    return check

def check_executable(file_path):
    """Verifica que un archivo sea ejecutable."""
    def check():
        path = Path(file_path)
        return path.exists() and path.stat().st_mode & 0o111
    return check

def check_env_var(var_name):
    """Verifica que una variable de entorno est√© configurada."""
    def check():
        import os
        return os.getenv(var_name) is not None
    return check

def main():
    print(f"{BLUE}{'='*80}{NC}")
    print(f"{BLUE}VALIDACI√ìN DE CONFIGURACI√ìN PARA TESIS{NC}")
    print(f"{BLUE}{'='*80}{NC}\n")

    all_checks = []

    # Secci√≥n 1: Archivos del Sistema
    print(f"{YELLOW}üìÅ Archivos del Sistema{NC}")
    all_checks.append(check_item("run_benchmark.py", check_file_exists("run_benchmark.py")))
    all_checks.append(check_item("run_complete_evaluation.sh", check_file_exists("run_complete_evaluation.sh")))
    all_checks.append(check_item("generate_thesis_plots.py", check_file_exists("generate_thesis_plots.py")))
    all_checks.append(check_item("analyze_robustness.py", check_file_exists("analyze_robustness.py")))
    all_checks.append(check_item("analyze_results.py", check_file_exists("analyze_results.py")))
    print()

    # Secci√≥n 2: Arquitecturas
    print(f"{YELLOW}üèóÔ∏è  Arquitecturas Agentivas{NC}")
    all_checks.append(check_item("ReAct", check_file_exists("src/benchmark_agent/architectures/react.py")))
    all_checks.append(check_item("Plan-Then-Act", check_file_exists("src/benchmark_agent/architectures/plan_then_act.py")))
    all_checks.append(check_item("Reference", check_file_exists("src/benchmark_agent/architectures/reference.py")))
    all_checks.append(check_item("Reflexion", check_file_exists("src/benchmark_agent/architectures/reflexion.py")))
    print()

    # Secci√≥n 3: Scripts Ejecutables
    print(f"{YELLOW}‚öôÔ∏è  Permisos de Ejecuci√≥n{NC}")
    all_checks.append(check_item("run_complete_evaluation.sh es ejecutable", 
                                 check_executable("run_complete_evaluation.sh")))
    all_checks.append(check_item("generate_thesis_plots.py es ejecutable", 
                                 check_executable("generate_thesis_plots.py")))
    print()

    # Secci√≥n 4: Dependencias Python
    print(f"{YELLOW}üêç Dependencias Python{NC}")
    all_checks.append(check_item("langchain", check_module("langchain")))
    all_checks.append(check_item("openai", check_module("openai")))
    all_checks.append(check_item("pandas", check_module("pandas")))
    all_checks.append(check_item("matplotlib", check_module("matplotlib")))
    all_checks.append(check_item("seaborn", check_module("seaborn")))
    all_checks.append(check_item("numpy", check_module("numpy")))
    print()

    # Secci√≥n 5: Variables de Entorno (Opcional)
    print(f"{YELLOW}üîë Variables de Entorno (Opcional){NC}")
    has_azure_key = check_item("AZURE_OPENAI_API_KEY", check_env_var("AZURE_OPENAI_API_KEY"))
    has_azure_endpoint = check_item("AZURE_OPENAI_ENDPOINT", check_env_var("AZURE_OPENAI_ENDPOINT"))
    
    if not has_azure_key or not has_azure_endpoint:
        print(f"{YELLOW}   ‚ö†Ô∏è  Las credenciales de Azure se pueden configurar en run_benchmark.py{NC}")
    print()

    # Secci√≥n 6: Directorios
    print(f"{YELLOW}üìÇ Estructura de Directorios{NC}")
    all_checks.append(check_item("src/benchmark_agent/", check_file_exists("src/benchmark_agent/")))
    all_checks.append(check_item("scenarios/", check_file_exists("scenarios/")))
    
    # Crear directorio de resultados si no existe
    benchmark_results_existed = Path("benchmark_results").exists()
    Path("benchmark_results").mkdir(exist_ok=True)
    all_checks.append(check_item("benchmark_results/", check_file_exists("benchmark_results/")))
    
    if not benchmark_results_existed:
        print(f"   {YELLOW}‚ö†Ô∏è  Directorio benchmark_results/ creado autom√°ticamente{NC}")
    print()

    # Resumen final
    print(f"{BLUE}{'='*80}{NC}")
    passed = sum(all_checks)
    total = len(all_checks)
    
    if passed == total:
        print(f"{GREEN}‚úÖ VALIDACI√ìN EXITOSA: {passed}/{total} checks pasaron{NC}")
        print(f"\n{GREEN}üéâ ¬°Todo listo para ejecutar la evaluaci√≥n completa!{NC}")
        print(f"\n{BLUE}Pr√≥ximo paso:{NC}")
        print(f"   ./run_complete_evaluation.sh")
        return 0
    else:
        print(f"{RED}‚ùå VALIDACI√ìN FALLIDA: {passed}/{total} checks pasaron{NC}")
        print(f"\n{YELLOW}Correcciones necesarias:{NC}")
        
        # Sugerencias de correcci√≥n
        if not check_module("matplotlib")():
            print(f"   ‚Ä¢ Instalar dependencias de gr√°ficos:")
            print(f"     pip install matplotlib pandas seaborn numpy")
        
        if not check_executable("run_complete_evaluation.sh")():
            print(f"   ‚Ä¢ Hacer scripts ejecutables:")
            print(f"     chmod +x run_complete_evaluation.sh generate_thesis_plots.py")
        
        return 1

if __name__ == "__main__":
    sys.exit(main())
